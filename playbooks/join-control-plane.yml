---
# Playbook for joining secondary control plane nodes to the cluster

- name: Join secondary control plane nodes
  hosts: k8s_master_secondary
  become: true
  gather_facts: true
  vars:
    master_node: "{{ groups['k8s_master_init'][0] }}"
  tasks:
    - name: Check if node is already joined
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf
      
    - name: Check if kubelet is active
      service:
        name: kubelet
        state: started
      register: kubelet_status
      ignore_errors: true
      
    - name: Stop kubelet if running to prevent port conflicts
      service:
        name: kubelet
        state: stopped
      when: not kubelet_conf.stat.exists and kubelet_status.status.ActiveState == "active"
      
    - name: Reset any previous kubernetes installation
      shell: kubeadm reset --force
      when: not kubelet_conf.stat.exists
      register: reset_result
      ignore_errors: true
      
    - name: Clean up network interfaces if needed
      shell: |
        ip link delete cni0 || true
        ip link delete flannel.1 || true
      ignore_errors: true
      when: not kubelet_conf.stat.exists and reset_result is changed
      
    - name: Generate new certificate and token on master
      delegate_to: "{{ master_node }}"
      block:
        - name: Remove existing certificate secrets if present
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret -n kube-system kubeadm-certs &>/dev/null && kubectl --kubeconfig=/etc/kubernetes/admin.conf delete secret -n kube-system kubeadm-certs || echo "No existing certs to delete"
          ignore_errors: true
          
        - name: Wait a moment before creating new certificates
          pause:
            seconds: 3
            
        - name: Upload fresh certificates to cluster with explicit TTL
          shell: kubeadm init phase upload-certs --upload-certs --certificate-key=$(openssl rand -hex 32) --v=5
          register: certs_upload
          
        - name: Display certificate upload details
          debug:
            var: certs_upload.stdout_lines
            
        - name: Extract certificate key
          set_fact:
            cert_key: "{{ certs_upload.stdout_lines[-1] }}"
          
        - name: Create bootstrap token
          shell: kubeadm token create --ttl 4h
          register: token_output
          
        - name: Extract token
          set_fact:
            bootstrap_token: "{{ token_output.stdout }}"
            
        - name: Get CA certificate hash
          shell: openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
          register: ca_hash
          
        - name: Validate certificate key format
          set_fact:
            clean_cert_key: "{{ cert_key | regex_replace('[^a-f0-9]', '') }}"
          
        - name: Verify certificate key length (should be 64 hex characters)
          fail:
            msg: "Certificate key is invalid format or length: {{ cert_key }}"
          when: clean_cert_key | length != 64
            
        - name: Verify certificate secret exists in kube-system namespace
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret -n kube-system kubeadm-certs -o name
          register: cert_secret_check
          failed_when: cert_secret_check.rc != 0
          
        - name: Create complete join command
          set_fact:
            control_plane_join_command: "kubeadm join 192.168.1.100:6443 --token {{ bootstrap_token }} --discovery-token-ca-cert-hash sha256:{{ ca_hash.stdout }} --control-plane --certificate-key {{ clean_cert_key }} --v=5"
      
    - name: Check if node is already joined
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf
      
    - name: Wait for first control plane API server to be ready
      delegate_to: "{{ master_node }}"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw='/readyz' || echo "API not ready"
      register: api_status
      until: api_status.stdout == "ok"
      retries: 30
      delay: 10
      changed_when: false
    
    - name: Join control plane node to the cluster
      shell: "{{ control_plane_join_command }}"
      args:
        creates: /etc/kubernetes/kubelet.conf
      when: not kubelet_conf.stat.exists
      register: join_result
      ignore_errors: true
      
    - name: Display join command output
      debug:
        var: join_result.stdout_lines
      when: join_result.stdout_lines is defined
      
    - name: Display join command errors
      debug:
        var: join_result.stderr_lines
      when: join_result.stderr_lines is defined
      
    - name: Check if join succeeded
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: join_success
      
    # Enhanced task to diagnose cert issues if join failed
    - name: Diagnose certificate issues when join fails
      delegate_to: "{{ master_node }}"
      block:
        - name: Ensure kubeadm-certs secret is deleted cleanly
          shell: |
            kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret -n kube-system kubeadm-certs -o name 2>/dev/null && \
            kubectl --kubeconfig=/etc/kubernetes/admin.conf delete secret -n kube-system kubeadm-certs && \
            echo "Deleted existing cert secret" || echo "No certs to delete"
          register: cert_delete_result
          
        - name: Wait longer after certificate deletion
          pause:
            seconds: 10
          when: "'Deleted existing cert secret' in cert_delete_result.stdout"
          
        - name: Re-create certificates with verbose logging and direct key extraction
          shell: |
            CERT_KEY=$(openssl rand -hex 32)
            echo "Using certificate key: $CERT_KEY"
            kubeadm init phase upload-certs --upload-certs --certificate-key=$CERT_KEY --v=5
            
            # Double check the secret exists
            if kubectl --kubeconfig=/etc/kubernetes/admin.conf get secret -n kube-system kubeadm-certs &>/dev/null; then
              echo "Certificate secret created successfully"
            else
              echo "ERROR: Certificate secret creation failed!"
            fi
            
            # Output the certificate key in a special format for easy extraction
            echo "EXTRACTED_CERT_KEY:$CERT_KEY"
          register: debug_cert_upload
          
        - name: Display verbose certificate details
          debug:
            var: debug_cert_upload
            
        - name: Extract new certificate key from verbose output
          set_fact:
            cert_key: "{{ debug_cert_upload.stdout_lines[-3] }}"
            
        - name: Create new bootstrap token
          shell: kubeadm token create --ttl 2h
          register: new_token_output
          
        - name: Verify token created successfully
          debug: 
            msg: "Token created: {{ new_token_output.stdout }}"
            
        - name: Extract new token
          set_fact:
            bootstrap_token: "{{ new_token_output.stdout }}"
            
        - name: Update join command with new token and certificate key
          set_fact:
            control_plane_join_command: "kubeadm join 192.168.1.100:6443 --token {{ bootstrap_token }} --discovery-token-ca-cert-hash sha256:{{ ca_hash.stdout }} --control-plane --certificate-key {{ cert_key }}" 
      when: join_result is failed
      
    - name: Retry join with proper cleanup and preflight error handling if first attempt failed
      block:
        - name: Display updated join command
          debug:
            msg: "Will attempt to join with command: {{ control_plane_join_command }}"
            
        # First properly cleanup any remnants of previous attempts with more thorough approach
        - name: Clean previous failed join attempt
          shell: |
            # Stop services first
            systemctl stop kubelet || true
            
            # Kill any stray processes
            pkill -f kubelet || true
            sleep 2
            
            # Clean configuration files
            rm -rf /etc/kubernetes/manifests/* || true
            rm -f /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ca.crt || true
            find /etc/kubernetes/pki/ -type f -not -path "*/\.*" -delete || true
            
            # Full reset with forced cleanup
            kubeadm reset --force
            
            # Clean network config that might cause issues
            rm -rf /etc/cni/net.d/* || true
            
            # Return success regardless of individual command results
            exit 0
          ignore_errors: true
        
        # Check for and kill any lingering kubelet processes
        - name: Check for running kubelet processes
          shell: "pgrep -f kubelet || echo 'No kubelet process'"
          register: kubelet_processes
          changed_when: false
        
        - name: Force kill any lingering kubelet processes with proper error handling
          block:
            - name: Attempt graceful kubelet termination first
              shell: "pkill -TERM -f kubelet || true"
              when: "'No kubelet process' not in kubelet_processes.stdout"
              
            - name: Wait for kubelet to terminate
              pause:
                seconds: 5
              when: "'No kubelet process' not in kubelet_processes.stdout"
              
            - name: Check if kubelet is still running
              shell: "pgrep -f kubelet || echo 'No kubelet process'"
              register: kubelet_check_after_term
              changed_when: false
              
            - name: Force kill kubelet if still running
              shell: "pkill -9 -f kubelet || true"
              when: "'No kubelet process' not in kubelet_check_after_term.stdout"
              
            - name: Final verification - kubelet shouldn't be running
              shell: "pgrep -f kubelet || echo 'No kubelet process'"
              register: final_kubelet_check
              changed_when: false
              
            - name: Log warning if kubelet still running
              debug:
                msg: "WARNING: Unable to terminate kubelet process. This might cause issues."
              when: "'No kubelet process' not in final_kubelet_check.stdout"
        
        # Make sure port 10250 is free
        - name: Check if port 10250 is in use
          shell: "netstat -tulpn | grep 10250 || echo 'Port free'"
          register: port_check
          changed_when: false
        
        - name: Force kill process using port 10250 if needed
          shell: "fuser -k 10250/tcp || true"
          when: "'Port free' not in port_check.stdout"
          
        - name: Wait for port to be released
          pause:
            seconds: 5
          when: "'Port free' not in port_check.stdout"
            
        # Restart containerd if needed
        - name: Restart containerd
          systemd:
            name: containerd
            state: restarted
          
        # Start kubelet service explicitly with daemon-reload
        - name: Start kubelet service explicitly
          systemd:
            name: kubelet
            state: restarted
            daemon_reload: yes
            enabled: yes
            
        - name: Give kubelet time to stabilize
          pause:
            seconds: 10
            
        # Retry join command with ignore-preflight-errors for port conflicts
        - name: Check kubelet service status before join
          shell: |
            systemctl status kubelet || echo "Kubelet status check failed"
            journalctl -xeu kubelet --no-pager -n 50 || echo "No kubelet logs available"
          register: kubelet_diagnostics
          ignore_errors: true
      
        - name: Ensure kubelet service is properly configured
          shell: |
            # Check kubelet configuration
            mkdir -p /var/lib/kubelet
            systemctl daemon-reload
            systemctl enable kubelet
            systemctl restart containerd
            
            # Wait for containerd to stabilize
            sleep 5
            
            # Test if kubelet can start properly without k8s components
            systemctl restart kubelet
            sleep 5
            systemctl status kubelet || echo "Kubelet failed to start"
          register: kubelet_prep
          ignore_errors: true
            
        - name: Retry join command with preflight errors ignored and extended verbosity
          shell: "{{ control_plane_join_command }} --ignore-preflight-errors=Port-10250,FileAvailable--etc-kubernetes-kubelet.conf,DirAvailable--var-lib-etcd --v=5"
          register: retry_join_result
          environment:
            KUBEADM_IGNORE_PREFLIGHT_CHECKS: "all"
      when: not join_success.stat.exists and join_result is failed
      
    - name: Wait for API server manifest to be created
      pause:
        seconds: 45
      when: join_result is changed or (retry_join_result is defined and retry_join_result is changed)
      
    # Enhanced API server manifest handling
    - name: Check if kube-apiserver manifest exists
      stat:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
      register: api_manifest_stat
      when: join_result is changed
      
    - name: Check if kube-apiserver backup exists
      stat:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
      register: api_backup_stat
      when: join_result is changed and not api_manifest_stat.stat.exists
      
    - name: Restore kube-apiserver manifest from backup if needed
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        remote_src: yes
        mode: '0644'
      when: join_result is changed and not api_manifest_stat.stat.exists and api_backup_stat.stat.exists
      register: restored_api_manifest
      
    - name: Wait for API server to restart after restoring manifest
      pause:
        seconds: 30
      when: restored_api_manifest is changed
      
    # If API manifest exists, back it up before making any changes
    - name: Backup kube-apiserver manifest if it exists
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
        remote_src: yes
        mode: '0644'
      when: join_result is changed and api_manifest_stat.stat.exists
      register: api_backup_after_join
      
    # Verify API server port configuration
    - name: Verify API server port configuration
      shell: "grep -o 'secure-port=6444' /etc/kubernetes/manifests/kube-apiserver.yaml || echo 'not-configured'"
      register: api_port_check
      ignore_errors: true
      when: join_result is changed and (api_manifest_stat.stat.exists or restored_api_manifest is changed)
      
    # Update port configuration using in-place modifications if needed
    - name: Update API server secure port in manifest (in-place)
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '--secure-port=6443'
        replace: '--secure-port=6444'
      when: join_result is changed and api_port_check.stdout == 'not-configured'
      register: api_port_update
      
    - name: Update all health check probe ports in API server manifest (in-place)
      replace:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        regexp: '([ ]{8}port: )6443(\n[ ]{8}scheme: HTTPS)'
        replace: '\16444\2'
      when: join_result is changed and api_port_check.stdout == 'not-configured'
      register: api_probe_update
      
    # Final verification and emergency restore if needed
    - name: Verify kube-apiserver manifest still exists after updates
      stat:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
      register: final_api_manifest_stat
      when: join_result is changed
      
    - name: Emergency restore from backup if manifest was lost during updates
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        remote_src: yes
        mode: '0644'
      when: join_result is changed and not final_api_manifest_stat.stat.exists and (api_backup_after_join is changed or api_backup_stat.stat.exists)
      register: emergency_restore
      
    - name: Create .kube directory for user
      file:
        path: "/home/{{ ansible_user }}/.kube"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'
      when: not kubelet_conf.stat.exists
      
    - name: Copy kubeconfig to user home directory
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "/home/{{ ansible_user }}/.kube/config"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0600'
        remote_src: yes
      when: not kubelet_conf.stat.exists