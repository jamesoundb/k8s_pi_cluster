---
# CNI Network Plugin Installation Playbook
# This playbook handles the installation of Flannel CNI after the control plane initialization
# Run this after bootstrap-control-plane.yml has successfully completed

- name: Install Flannel CNI Network Plugin
  hosts: k8s_master_init
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
    k8s_username: "{{ ansible_user }}"
    api_endpoint_address: "{{ hostvars[groups['k8s_master_init'][0]]['ansible_default_ipv4']['address'] }}"
    control_plane_endpoint: "192.168.1.100"
    api_server_port: "6443"
    keepalived_auth_pass: "K8sVIP"
  
  tasks:
    # Verify and fix etcd before proceeding
    - name: Verify etcd is healthy before CNI deployment
      include_role:
        name: etcd
        tasks_from: verify
      
    # First verify that we have a running API server with proper authentication
    - name: Check if admin.conf exists
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf_stat
    
    - name: Display admin.conf status
      debug:
        msg: "admin.conf exists: {{ admin_conf_stat.stat.exists }}"
    
    - name: Fail if admin.conf is missing
      fail:
        msg: "Error: /etc/kubernetes/admin.conf is missing. The Kubernetes API server may not have initialized properly."
      when: not admin_conf_stat.stat.exists
    
    # Create kubeconfig directory for the user if needed
    - name: Ensure .kube directory exists for root
      file:
        path: "/root/.kube"
        state: directory
        mode: '0755'

    - name: Ensure .kube directory exists for regular user
      file:
        path: "/home/{{ k8s_username }}/.kube"
        state: directory
        owner: "{{ k8s_username }}"
        group: "{{ k8s_username }}"
        mode: '0755'
      
    - name: Copy admin.conf to user's kubeconfig if needed
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ item.dest }}"
        remote_src: yes
        owner: "{{ item.owner }}"
        group: "{{ item.group }}"
        mode: '0600'
      with_items:
        - { dest: "/root/.kube/config", owner: "root", group: "root" }
        - { dest: "/home/{{ k8s_username }}/.kube/config", owner: "{{ k8s_username }}", group: "{{ k8s_username }}" }
      when: admin_conf_stat.stat.exists
    
    # Make sure HAProxy and keepalived are correctly configured before proceeding
    - name: Verify and fix HAProxy and keepalived configuration
      block:
        # Fix HAProxy configuration if needed
        - name: Check HAProxy configuration
          stat:
            path: /etc/haproxy/haproxy.cfg
          register: haproxy_cfg
          
        - name: Ensure HAProxy backends use port 6443
          replace:
            path: /etc/haproxy/haproxy.cfg
            regexp: '(server\s+apiserver\d+\s+[\d\.]+):6444'
            replace: '\1:6443'
          when: haproxy_cfg.stat.exists
          register: haproxy_updated
          
        - name: Restart HAProxy if configuration changed
          systemd:
            name: haproxy
            state: restarted
            enabled: yes
          when: haproxy_updated is defined and haproxy_updated.changed
          
        # Fix keepalived configuration if needed  
        - name: Check keepalived configuration
          stat:
            path: /etc/keepalived/keepalived.conf
          register: keepalived_cfg
          
        - name: Ensure keepalived uses the correct authentication password
          replace:
            path: /etc/keepalived/keepalived.conf
            regexp: 'auth_pass\s+\w+'
            replace: 'auth_pass {{ keepalived_auth_pass }}'
          when: keepalived_cfg.stat.exists
          register: keepalived_updated
          
        - name: Restart keepalived if configuration changed
          systemd:
            name: keepalived
            state: restarted
            enabled: yes
          when: keepalived_updated is defined and keepalived_updated.changed
          
        - name: Wait for services to stabilize
          pause:
            seconds: 5
          when: haproxy_updated.changed or keepalived_updated.changed
    
    # Wait for API server to be ready with robust checking
    - name: Wait for API server to be ready
      block:
        - name: Check if API server is accessible via VIP
          shell: curl -k https://{{ control_plane_endpoint }}:{{ api_server_port | default('6443') }}/healthz || echo "not ready"
          register: vip_api_health
          retries: 5
          delay: 10
          until: vip_api_health.stdout == "ok"
          ignore_errors: true
          
        - name: Check if API server is accessible via localhost
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw='/healthz' || echo "not ready"
          register: local_api_health
          retries: 12
          delay: 5
          until: local_api_health.stdout == "ok"
          ignore_errors: true
          
        - name: Set combined health status
          set_fact:
            combined_health: "{{ (vip_api_health.stdout is defined and vip_api_health.stdout == 'ok') or (local_api_health.stdout is defined and local_api_health.stdout == 'ok') }}"
    
    - name: Display API server health status
      debug:
        msg: >
          API server health status: {{ combined_health | default(false) | ternary('healthy', 'not healthy') }}
          VIP endpoint ({{ control_plane_endpoint }}:{{ api_server_port | default('6443') }}): {{ vip_api_health.stdout | default('not available') }}
          Local endpoint: {{ local_api_health.stdout | default('not available') }}
    
    - name: Verify nodes are registered
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
      register: nodes
      ignore_errors: true
      
    - name: Display nodes
      debug:
        var: nodes.stdout_lines
    
    # Instead of moving manifest files around, use our sync_apiserver_config task
    - name: Synchronize API server configuration
      include_role:
        name: ../roles/kube_apiserver
        tasks_from: sync_apiserver_config
    
    # Check and update all kubeconfig files - this time with correct port
    - name: Gather all kubeconfig files
      find:
        paths: /etc/kubernetes
        patterns: "*.conf"
      register: kubeconfig_files

    - name: Update kubeconfig files to use correct port
      replace:
        path: "{{ item.path }}"
        regexp: 'server: https://[0-9.]+:6444'
        replace: 'server: https://{{ control_plane_endpoint }}:{{ api_server_port }}'
      with_items: "{{ kubeconfig_files.files }}"
      loop_control:
        label: "{{ item.path }}"
      register: kubeconfig_updates

    # Restart kubelet if any kubeconfig files were updated
    - name: Restart kubelet if configurations were updated
      systemd:
        name: kubelet
        state: restarted
      when: kubeconfig_updates.changed
      register: kubelet_restarted
      
    - name: Wait for kubelet to settle after restart
      pause:
        seconds: 10
      when: kubelet_restarted is changed
      
    # Download Flannel manifest
    - name: Create temporary directory for CNI manifests
      file:
        path: /tmp/flannel
        state: directory
        mode: '0755'
    
    - name: Download Flannel manifest using curl
      shell: curl -L --insecure https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml -o /tmp/flannel/kube-flannel.yml
      register: flannel_download
      retries: 3
      delay: 5
      until: flannel_download.rc == 0
      
    - name: Verify download succeeded
      stat:
        path: /tmp/flannel/kube-flannel.yml
      register: flannel_file
      
    - name: Create backup of original Flannel manifest
      copy:
        src: /tmp/flannel/kube-flannel.yml
        dest: /tmp/flannel/kube-flannel.yml.orig
        remote_src: yes
        mode: '0644'
      when: flannel_file.stat.exists
      
    # Update the Flannel manifest with correct pod CIDR
    - name: Update Pod CIDR in Flannel manifest
      replace:
        path: /tmp/flannel/kube-flannel.yml
        regexp: 'net-conf.json: \|\n\s+{\n\s+"Network": "10\.[0-9]+\.[0-9]+\.[0-9]+/16"'
        replace: 'net-conf.json: |\n      {\n        "Network": "{{ pod_network_cidr }}"'
      when: flannel_download is succeeded
    
    # Apply the CNI network plugin
    - name: Apply Flannel CNI - first attempt
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/flannel/kube-flannel.yml
      register: flannel_apply
      ignore_errors: true
    
    - name: Display Flannel application result
      debug:
        var: flannel_apply.stdout_lines
    
    # Add retry logic for CNI application
    - name: Wait and retry if Flannel apply failed
      block:
        - name: Pause before retrying
          pause:
            seconds: 15
          
        - name: Retry Flannel application
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/flannel/kube-flannel.yml
          register: flannel_retry
          
        - name: Display retry result
          debug:
            var: flannel_retry.stdout_lines
      when: flannel_apply.rc != 0
      
    # Wait for Flannel pods to be ready
    - name: Wait for Flannel pods to be ready
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-flannel -l app=flannel | grep -c "Running" || echo "0"
      register: flannel_pods_ready
      retries: 20
      delay: 10
      until: flannel_pods_ready.stdout | int > 0
      ignore_errors: true
    
    - name: Display Flannel pod status
      debug:
        msg: "Flannel pods running: {{ flannel_pods_ready.stdout }}"
    
    # Verify pod and node networking is working
    - name: Get node status
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
      register: node_status
      
    - name: Display node network status
      debug:
        var: node_status.stdout_lines
    
    # If we get this far, remove master node taints to allow scheduling (optional)
    - name: Remove NoSchedule taint from control plane
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/control-plane- || echo "No taints to remove"
      when: true  # Change to a condition if you want to keep taints in some cases
    
    # Final verification of network connectivity
    - name: Verify kube-dns/CoreDNS is Running
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l k8s-app=kube-dns
      register: dns_status
      
    - name: Display DNS status
      debug:
        var: dns_status.stdout_lines
    
    - name: Create test pod to verify networking
      block:
        - name: Create a test pod to verify networking
          copy:
            dest: /tmp/test-pod.yaml
            content: |
              apiVersion: v1
              kind: Pod
              metadata:
                name: network-test
                namespace: default
              spec:
                containers:
                - name: network-test
                  image: busybox:1.28
                  command:
                    - sleep
                    - "3600"
                restartPolicy: Always
            mode: '0644'
        
        - name: Apply test pod
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/test-pod.yaml
          register: test_pod_apply
          ignore_errors: true
          
        - name: Wait for test pod to be Running
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pod network-test -o jsonpath='{.status.phase}'
          register: test_pod_status
          retries: 15
          delay: 10
          until: test_pod_status.stdout == "Running"
          ignore_errors: true
        
        - name: Test internal DNS resolution (kube-dns/CoreDNS)
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf exec network-test -- nslookup kubernetes.default.svc.cluster.local
          register: dns_test
          ignore_errors: true
          when: test_pod_status.stdout == "Running"
        
        - name: Display DNS test results
          debug:
            var: dns_test.stdout_lines
          when: dns_test is defined and dns_test.stdout_lines is defined
        
        - name: Test pod network connectivity
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf exec network-test -- ping -c 4 8.8.8.8
          register: network_test
          ignore_errors: true
          when: test_pod_status.stdout == "Running"
          
        - name: Display network test results
          debug:
            var: network_test.stdout_lines
          when: network_test is defined and network_test.stdout_lines is defined
          
        - name: Clean up test pod
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf delete pod network-test
          ignore_errors: true
      
    - name: Final CNI status
      debug:
        msg: "CNI plugin deployment completed. Network connectivity status: {{ (network_test is defined and network_test.rc == 0) | ternary('Success', 'Incomplete') }}"
    
    # Add comprehensive recovery steps if API server is not healthy
    - name: Attempt to fix API server if not healthy
      block:
        - name: Check kubelet status
          service:
            name: kubelet
            state: started
          register: kubelet_status
          
        - name: Check API server pod manifest
          stat:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml
          register: apiserver_manifest
          
        - name: Check HAProxy status
          service:
            name: haproxy
            state: started
          register: haproxy_status
          
        - name: Check keepalived status
          service:
            name: keepalived
            state: started
          register: keepalived_status
          
        - name: Display component status
          debug:
            msg: >
              Component status:
              - Kubelet: {{ kubelet_status.status.ActiveState | default('unknown') }}
              - API server manifest: {{ apiserver_manifest.stat.exists | default(false) | ternary('exists', 'missing') }}
              - HAProxy: {{ haproxy_status.status.ActiveState | default('unknown') }}
              - Keepalived: {{ keepalived_status.status.ActiveState | default('unknown') }}
        
        # Verify etcd connectivity from API server
        - name: Verify etcd connectivity
          shell: >
            ETCDCTL_API=3 etcdctl 
            --cacert=/etc/kubernetes/pki/etcd/ca.crt 
            --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt 
            --key=/etc/kubernetes/pki/apiserver-etcd-client.key
            --endpoints=https://127.0.0.1:2379 
            endpoint health || echo "etcd health check failed"
          register: etcd_api_conn
          ignore_errors: true

        - name: Display etcd connectivity
          debug:
            msg: "etcd connectivity from API server: {{ etcd_api_conn.stdout | default('unknown') }}"
              
        # Enhanced recovery procedure
        - name: Perform comprehensive recovery procedure
          block:
            # 1. Fix configuration files first
            - name: Apply configuration sync to ensure consistent port settings
              include_role:
                name: ../roles/kube_apiserver
                tasks_from: sync_apiserver_config
              when: apiserver_manifest.stat.exists

            # 2. Verify certificates
            - name: Check API server certificate permissions
              file:
                path: "/etc/kubernetes/pki/{{ item }}"
                mode: "0600"
                state: file
              loop:
                - apiserver.key
                - apiserver-etcd-client.key
                - apiserver-kubelet-client.key
              ignore_errors: true
              when: apiserver_manifest.stat.exists

            # 3. Fix HAProxy and Keepalived
            - name: Ensure HAProxy configuration is correct
              template:
                src: ../roles/kube_apiserver/templates/haproxy.cfg.j2
                dest: /etc/haproxy/haproxy.cfg
                mode: '0644'
              register: haproxy_cfg_updated
              
            - name: Restart HAProxy with correct config
              service:
                name: haproxy
                state: restarted
              when: haproxy_cfg_updated.changed or haproxy_status.status.ActiveState != 'active'

            - name: Ensure keepalived configuration is correct
              template:
                src: ../roles/kube_apiserver/templates/keepalived.conf.j2
                dest: /etc/keepalived/keepalived.conf
                mode: '0644'
              register: keepalived_cfg_updated
              
            - name: Restart keepalived with correct config
              service:
                name: keepalived
                state: restarted
              when: keepalived_cfg_updated.changed or keepalived_status.status.ActiveState != 'active'

            # 4. Fix systemd unit files if needed
            - name: Set correct kubelet environment
              copy:
                dest: /etc/default/kubelet
                content: |
                  KUBELET_EXTRA_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock
                mode: '0644'
              register: kubelet_env_updated

            # 5. Regenerate API server manifest if missing
            - name: Regenerate API server manifest if missing
              command: kubeadm init phase control-plane apiserver --config=/etc/kubernetes/kubeadm/kubeadm-init.yaml
              when: not apiserver_manifest.stat.exists
              ignore_errors: true
              
            # 6. Restart kubelet as the final step
            - name: Restart kubelet to pick up all changes
              service:
                name: kubelet
                state: restarted
                daemon_reload: yes
              when: kubelet_env_updated.changed or not apiserver_manifest.stat.exists
                
            # 7. Wait longer for services to stabilize
            - name: Wait for services to stabilize after comprehensive recovery
              pause:
                seconds: 60

            # 8. Check API server health with longer retry period
            - name: Check API server health after comprehensive recovery
              shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw='/healthz' || echo "not ready"
              register: api_health_comprehensive
              retries: 15
              delay: 10
              until: api_health_comprehensive.stdout == "ok"
              ignore_errors: true
              
            - name: Display API server health after comprehensive recovery
              debug:
                msg: "API server health after comprehensive recovery: {{ api_health_comprehensive.stdout | default('still not available') }}"
              
            # 9. Additional diagnostics if still not working
            - name: Gather extended diagnostics if API server still not healthy
              block:
                - name: Check kubelet logs
                  shell: journalctl -u kubelet --no-pager -n 100 | grep -i error
                  register: kubelet_err_logs
                  ignore_errors: true
                  
                - name: Display kubelet error logs
                  debug:
                    var: kubelet_err_logs.stdout_lines
                    
                - name: Check container runtime status
                  shell: crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube-apiserver
                  register: apiserver_container
                  ignore_errors: true
                  
                - name: Display API server container status
                  debug:
                    var: apiserver_container.stdout_lines
              when: api_health_comprehensive.stdout != "ok"
      when: not combined_health | default(false)
