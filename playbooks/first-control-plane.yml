---
# First Control Plane Initialization Playbook
# This handles the bootstrap of the first Kubernetes control plane node
# with proper kubelet configuration and API server on port 6444

- name: Initialize First Kubernetes Control Plane Node
  hosts: k8s_master_init
  become: true
  vars:
    k8s_username: "{{ ansible_user }}"
    pod_network_cidr: "10.244.0.0/16"
    service_cidr: "10.245.0.0/16"
    control_plane_endpoint: "192.168.1.100"  # Virtual IP for HA control plane
    api_endpoint_address: "{{ ansible_host }}"  # Use the actual node IP
    kubernetes_version: "1.29.0"
  
  tasks:
    - name: Ensure containerd is running
      systemd:
        name: containerd
        state: started
        enabled: yes

    # Reset any existing Kubernetes configuration
    - name: Reset previous Kubernetes installations
      shell: kubeadm reset --force
      register: reset_result
      ignore_errors: true

    - name: Display kubeadm reset output
      debug:
        var: reset_result.stdout_lines
      when: reset_result.stdout_lines is defined

    - name: Clean up Kubernetes directories after reset
      shell: |
        rm -rf /etc/kubernetes/manifests/* || true
        rm -rf /var/lib/etcd/* || true
        rm -rf /etc/kubernetes/admin.conf || true
        rm -rf /etc/kubernetes/kubelet.conf || true
        rm -rf /home/{{ ansible_user }}/.kube/config || true
        rm -rf /var/lib/kubelet/config.yaml || true
        ip link delete cni0 || true
        ip link delete flannel.1 || true
      ignore_errors: true
      
    - name: Restart containerd after reset
      systemd:
        name: containerd
        state: restarted
      
    - name: Stop kubelet after reset
      systemd:
        name: kubelet
        state: stopped
      ignore_errors: true

    # Stop HAProxy temporarily to prevent port conflicts during initialization
    - name: Check if HAProxy is running and stop it if it is
      systemd:
        name: haproxy
        state: stopped
      ignore_errors: true
      
    # Check what's using port 6443/6444 before we start
    - name: Check what process is using port 6443
      shell: ss -tlnp | grep 6443 || echo "No process using port 6443"
      register: port_check_6443
      ignore_errors: true
      
    - name: Check what process is using port 6444
      shell: ss -tlnp | grep 6444 || echo "No process using port 6444"
      register: port_check_6444
      ignore_errors: true
      
    - name: Display processes using critical ports
      debug:
        msg: 
          - "Port 6443: {{ port_check_6443.stdout }}"
          - "Port 6444: {{ port_check_6444.stdout }}"
    
    # CRITICAL STEP 1: Create bootstrap kubelet configuration
    # This solves the chicken-and-egg problem where kubelet needs certs that don't exist yet
    - name: Ensure kubelet config directory exists
      file:
        path: /var/lib/kubelet
        state: directory
        mode: '0755'
    
    - name: Create bootstrap kubelet configuration
      copy:
        dest: /var/lib/kubelet/config.yaml
        content: |
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          authentication:
            anonymous:
              enabled: true
            webhook:
              enabled: false
            x509:
              clientCAFile: ""
          authorization:
            mode: AlwaysAllow
          clusterDomain: cluster.local
          clusterDNS:
          - 10.245.0.10
          resolvConf: /run/systemd/resolve/resolv.conf
          cgroupDriver: systemd
          failSwapOn: false
          runtimeRequestTimeout: 15m
        mode: '0644'
      
    # CRITICAL STEP 2: Update kubelet to use our bootstrap configuration
    - name: Update kubelet environment file
      lineinfile:
        path: /var/lib/kubelet/kubeadm-flags.env
        regexp: '^KUBELET_KUBEADM_ARGS='
        line: 'KUBELET_KUBEADM_ARGS="--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-manifest-path=/etc/kubernetes/manifests --config=/var/lib/kubelet/config.yaml"'
        create: yes
        mode: '0644'
      register: kubelet_env_updated
    
    - name: Restart kubelet with bootstrap configuration
      systemd:
        daemon-reload: yes
        name: kubelet
        state: restarted
    
    - name: Pause to let kubelet start
      pause:
        seconds: 10
      when: kubelet_env_updated is changed
    
    - name: Check kubelet status
      shell: "systemctl status kubelet --no-pager | head -20"
      register: kubelet_status
      ignore_errors: true
      
    - name: Display kubelet status
      debug:
        var: kubelet_status.stdout_lines
    
    # CRITICAL STEP 3: Generate kubeadm config that uses port 6444 for API server
    - name: Generate kubeadm configuration
      copy:
        dest: /etc/kubernetes/kubeadm-init-config.yaml
        content: |
          apiVersion: kubeadm.k8s.io/v1beta3
          kind: ClusterConfiguration
          kubernetesVersion: v{{ kubernetes_version }}
          controlPlaneEndpoint: "{{ control_plane_endpoint }}:6443"
          networking:
            podSubnet: "{{ pod_network_cidr }}"
            serviceSubnet: "{{ service_cidr }}"
          apiServer:
            extraArgs:
              secure-port: "6444"
              advertise-address: "{{ api_endpoint_address }}"
          etcd:
            local:
              dataDir: "/var/lib/etcd"
        owner: root
        group: root
        mode: '0644'
    
    # CRITICAL STEP 4: Initialize the cluster with kubeadm
    - name: Initialize Kubernetes cluster with kubeadm
      shell: kubeadm init --config=/etc/kubernetes/kubeadm-init-config.yaml --ignore-preflight-errors=Port-6443 --v=5 --upload-certs
      args:
        creates: /etc/kubernetes/admin.conf
      register: kubeadm_init
      ignore_errors: true
      
    - name: Display kubeadm init output if it failed
      debug:
        var: kubeadm_init.stdout_lines
      when: kubeadm_init is failed
    
    # Diagnostic collection if initialization failed
    - name: Collect diagnostic information if initialization failed
      block:
        - name: Check kubelet logs
          shell: journalctl -xeu kubelet | tail -n 50
          register: kubelet_logs
          ignore_errors: true
          
        - name: Display kubelet logs
          debug:
            var: kubelet_logs.stdout_lines
          when: kubelet_logs is defined
          
        - name: Check for kube-apiserver containers
          shell: crictl ps -a | grep kube-apiserver || echo "No kube-apiserver containers found"
          register: apiserver_containers
          ignore_errors: true
          
        - name: Display kube-apiserver containers
          debug:
            var: apiserver_containers.stdout_lines
          when: apiserver_containers is defined
          
        - name: Get logs from the most recent kube-apiserver container
          shell: |
            CONTAINER_ID=$(crictl ps -a | grep kube-apiserver | head -n 1 | awk '{print $1}')
            if [ ! -z "$CONTAINER_ID" ]; then
              crictl logs $CONTAINER_ID
            else
              echo "No kube-apiserver container ID found"
            fi
          register: apiserver_logs
          ignore_errors: true
          
        - name: Display kube-apiserver logs
          debug:
            var: apiserver_logs.stdout_lines
          when: apiserver_logs is defined
      when: kubeadm_init is failed
    
    # CRITICAL STEP 5: Update all kubeconfig files to use port 6444 for direct API server access
    - name: Create .kube directory for user
      file:
        path: "/home/{{ ansible_user }}/.kube"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'
    
    # Check if admin.conf exists first to avoid errors
    - name: Check if admin.conf exists
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf_stat
    
    - name: Copy kube config for user if it exists
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "/home/{{ ansible_user }}/.kube/config"
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0600'
      when: admin_conf_stat.stat.exists
    
    # Update user kubeconfig if it exists
    - name: Check if user kubeconfig exists
      stat:
        path: "/home/{{ ansible_user }}/.kube/config"
      register: user_kubeconfig
      
    - name: Backup and update user kubeconfig
      shell: |
        cp /home/{{ ansible_user }}/.kube/config /home/{{ ansible_user }}/.kube/config.orig || true
        sed -i "s|server: https://.*:6443|server: https://{{ api_endpoint_address }}:6444|g" /home/{{ ansible_user }}/.kube/config
      when: user_kubeconfig.stat.exists
      
    # Update admin.conf if it exists
    - name: Check if admin.conf exists
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf
      
    - name: Backup and update admin.conf
      shell: |
        cp /etc/kubernetes/admin.conf /etc/kubernetes/admin.conf.orig || true
        sed -i "s|server: https://.*:6443|server: https://{{ api_endpoint_address }}:6444|g" /etc/kubernetes/admin.conf
      when: admin_conf.stat.exists
      
    # Update kubelet.conf if it exists
    - name: Check if kubelet.conf exists
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf
      
    - name: Backup and update kubelet.conf
      shell: |
        cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.orig || true
        sed -i "s|server: https://.*:6443|server: https://{{ api_endpoint_address }}:6444|g" /etc/kubernetes/kubelet.conf
      when: kubelet_conf.stat.exists
      
    - name: Restart kubelet if kubelet.conf was updated
      systemd:
        name: kubelet
        state: restarted
      when: kubelet_conf.stat.exists
      
    # Update controller-manager.conf if it exists
    - name: Check if controller-manager.conf exists
      stat:
        path: /etc/kubernetes/controller-manager.conf
      register: cm_conf
      
    - name: Backup and update controller-manager.conf
      shell: |
        cp /etc/kubernetes/controller-manager.conf /etc/kubernetes/controller-manager.conf.orig || true
        sed -i "s|server: https://.*:6443|server: https://{{ api_endpoint_address }}:6444|g" /etc/kubernetes/controller-manager.conf
      when: cm_conf.stat.exists
      
    # Update scheduler.conf if it exists
    - name: Check if scheduler.conf exists
      stat:
        path: /etc/kubernetes/scheduler.conf
      register: scheduler_conf
      
    - name: Backup and update scheduler.conf
      shell: |
        cp /etc/kubernetes/scheduler.conf /etc/kubernetes/scheduler.conf.orig || true
        sed -i "s|server: https://.*:6443|server: https://{{ api_endpoint_address }}:6444|g" /etc/kubernetes/scheduler.conf
      when: scheduler_conf.stat.exists and admin_conf_stat.stat.exists
    
    # CRITICAL STEP 6: Update probe ports in kube-apiserver manifest
    - name: Wait for kube-apiserver manifest
      wait_for:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
        state: present
        timeout: 60
      ignore_errors: true
    
    # Check if kube-apiserver.yaml exists, if not restore from backup
    - name: Check if kube-apiserver manifest exists
      stat:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
      register: api_manifest_stat
      ignore_errors: true
    
    - name: Check if kube-apiserver backup exists
      stat:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
      register: api_backup_stat
      when: not api_manifest_stat.stat.exists
      ignore_errors: true
    
    - name: Restore kube-apiserver manifest from backup if needed
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        remote_src: yes
        mode: '0644'
      when: not api_manifest_stat.stat.exists and api_backup_stat.stat.exists
      register: restored_api_manifest
      ignore_errors: true
    
    - name: Wait for API server to restart after restoring manifest
      pause:
        seconds: 30
      when: restored_api_manifest is changed
    
    - name: Backup kube-apiserver manifest
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
        remote_src: yes
        mode: '0644'
      ignore_errors: true
      register: api_backup
      
    # Create a copy instead of moving the manifest
    - name: Copy kube-apiserver manifest for editing
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml
        dest: /etc/kubernetes/kube-apiserver.yaml.tmp
        remote_src: yes
        mode: '0644'
      when: api_backup is succeeded
      
    # Update all probe ports in the manifest
    - name: Update all probe ports in kube-apiserver manifest
      replace:
        path: /etc/kubernetes/kube-apiserver.yaml.tmp
        regexp: '([ ]{8}port: )6443(\n[ ]{8}scheme: HTTPS)'
        replace: '\16444\2'
      when: api_backup is succeeded
      
    # Update advertise address annotation if present
    - name: Update advertise address endpoint annotation
      replace:
        path: /etc/kubernetes/kube-apiserver.yaml.tmp
        regexp: '([ ]{4}kubeadm\.kubernetes\.io/kube-apiserver\.advertise-address\.endpoint: .*:)6443'
        replace: '\16444'
      when: api_backup is succeeded
      ignore_errors: true
      
    # Properly restore the manifest to trigger a restart
    - name: Copy edited kube-apiserver manifest back
      copy:
        src: /etc/kubernetes/kube-apiserver.yaml.tmp
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        remote_src: yes
        mode: '0644'
      when: api_backup is succeeded
      register: api_restore
      
    - name: Remove temporary manifest file
      file:
        path: /etc/kubernetes/kube-apiserver.yaml.tmp
        state: absent
      when: api_backup is succeeded
      
    - name: Verify kube-apiserver manifest exists after updates
      stat:
        path: /etc/kubernetes/manifests/kube-apiserver.yaml
      register: api_manifest_after_update
      
    - name: Emergency restore from backup if manifest was lost
      copy:
        src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        remote_src: yes
        mode: '0644'
      when: not api_manifest_after_update.stat.exists
      register: emergency_restore
      
    - name: Pause for kubelet to process manifest changes
      pause:
        seconds: 10
    
    # Wait for API server to start
    - name: Wait for kube-apiserver container
      shell: "until crictl ps | grep kube-apiserver; do sleep 5; echo 'Waiting for kube-apiserver container...'; done"
      args:
        executable: /bin/bash
      register: apiserver_container_check
      timeout: 120
      ignore_errors: true
      
    # Check if API server is listening on port 6444
    - name: Wait for API server to be listening
      wait_for:
        port: 6444
        host: "{{ api_endpoint_address }}"
        timeout: 180
        delay: 10
      register: apiserver_port_check
      ignore_errors: true
    
    - name: Display API server port status
      debug:
        msg: "API server listening on port 6444: {{ apiserver_port_check.state == 'started' }}"
    
    # Wait for API server to be responsive
    - name: Wait for API server to be responsive
      shell: "until kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes &>/dev/null; do sleep 5; echo 'Waiting for API server...'; done"
      args:
        executable: /bin/bash
      timeout: 180
      ignore_errors: true
      register: apiserver_ready
      
    - name: Display API server readiness status
      debug:
        msg: "API server is ready: {{ apiserver_ready is succeeded }}"
    
    # Start HAProxy after the API server is configured
    - name: Restart HAProxy to use with port 6444
      systemd:
        name: haproxy
        state: restarted
      
    - name: Wait for HAProxy to be ready
      wait_for:
        port: 6443
        host: "{{ control_plane_endpoint }}"
        timeout: 60
      ignore_errors: true
