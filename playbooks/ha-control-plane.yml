---
# High-Availability Control Plane Deployment Playbook

# Initialize the first control plane node
- name: Initialize First Control Plane Node
  hosts: k8s_master_init
  become: true
  vars:
    k8s_username: "{{ ansible_user }}"
  tasks:
    # Apply the kubeadm init configuration from the kube_apiserver role
    - name: Ensure kubelet is enabled
      systemd:
        name: kubelet
        enabled: yes

    - name: Initialize Kubernetes control plane with kubeadm
      command: kubeadm init --config=/etc/kubernetes/kubeadm/kubeadm-init.yaml --upload-certs
      args:
        creates: /etc/kubernetes/admin.conf
      register: kubeadm_init_result

    - name: Create .kube directory for user
      file:
        path: "/home/{{ k8s_username }}/.kube"
        state: directory
        owner: "{{ k8s_username }}"
        group: "{{ k8s_username }}"
        mode: '0755'
      
    - name: Copy kubeconfig to user home directory
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "/home/{{ k8s_username }}/.kube/config"
        owner: "{{ k8s_username }}"
        group: "{{ k8s_username }}"
        mode: '0600'
        remote_src: yes
    
    - name: Generate join command for additional control plane nodes
      command: kubeadm token create --print-join-command
      register: join_command
      changed_when: false

    - name: Generate certificate key for control plane join
      command: kubeadm init phase upload-certs --upload-certs
      register: cert_key_result
      changed_when: false
    
    - name: Extract certificate key
      set_fact:
        cert_key: "{{ cert_key_result.stdout_lines[-1] }}"
    
    - name: Store join command for control plane nodes
      set_fact:
        control_plane_join_command: "{{ join_command.stdout }} --control-plane --certificate-key {{ cert_key }}"
    
    - name: Save control plane join command to file
      copy:
        content: "#!/bin/bash\n{{ control_plane_join_command }}"
        dest: "/home/{{ k8s_username }}/control_plane_join_command.sh"
        owner: "{{ k8s_username }}"
        group: "{{ k8s_username }}"
        mode: '0700'

    - name: Save worker join command to file
      copy:
        content: "#!/bin/bash\n{{ join_command.stdout }}"
        dest: "/home/{{ k8s_username }}/worker_join_command.sh"
        owner: "{{ k8s_username }}"
        group: "{{ k8s_username }}"
        mode: '0700'

    - name: Fetch control plane join command to control machine
      fetch:
        src: "/home/{{ k8s_username }}/control_plane_join_command.sh"
        dest: "./control_plane_join_command.sh"
        flat: yes
    
    - name: Fetch worker join command to control machine
      fetch:
        src: "/home/{{ k8s_username }}/worker_join_command.sh"
        dest: "./worker_join_command.sh"
        flat: yes
        
    # Set up proper RBAC permissions for kubernetes-admin user with improved approach
    - name: Create kubernetes-admin ClusterRoleBinding manifest
      copy:
        content: |
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: kubernetes-admin-cluster-admin
          subjects:
          - kind: User
            name: kubernetes-admin
            apiGroup: rbac.authorization.k8s.io
          roleRef:
            kind: ClusterRole
            name: cluster-admin
            apiGroup: rbac.authorization.k8s.io
        dest: "/tmp/kubernetes-admin-rbac.yaml"
        mode: '0644'
      
    - name: Get bootstrap token
      shell: kubeadm token create
      register: bootstrap_token
      ignore_errors: true
      
    - name: Apply RBAC configuration (using dry-run approach)
      shell: |
        export KUBECONFIG=/etc/kubernetes/admin.conf
        kubectl create clusterrolebinding kubernetes-admin-cluster-admin --clusterrole=cluster-admin --user=kubernetes-admin --dry-run=client -o yaml | kubectl apply -f -
      register: rbac_result
      failed_when: rbac_result.rc != 0 and "AlreadyExists" not in rbac_result.stderr
      
    - name: Apply RBAC configuration (fallback method)
      command: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/kubernetes-admin-rbac.yaml
      register: rbac_fallback_result
      when: rbac_result is failed
      failed_when: rbac_fallback_result.rc != 0 and "AlreadyExists" not in rbac_fallback_result.stderr
      
    - name: Verify RBAC permissions
      shell: KUBECONFIG=/etc/kubernetes/admin.conf kubectl auth can-i get nodes
      register: auth_check
      ignore_errors: true
      
    - name: Display RBAC configuration results
      debug:
        msg: "RBAC configuration status: {{ 'Successful' if (rbac_result is succeeded or rbac_fallback_result is succeeded) and auth_check.stdout == 'yes' else 'Failed' }}"

# Join additional control plane nodes
- name: Join Additional Control Plane Nodes
  hosts: k8s_master_secondary
  become: true
  vars:
    k8s_username: "{{ ansible_user }}"
  tasks:
    - name: Copy control plane join command
      copy:
        src: "./control_plane_join_command.sh"
        dest: "/tmp/control_plane_join_command.sh"
        mode: '0700'
    
    - name: Join additional control plane node to the cluster
      command: /tmp/control_plane_join_command.sh
      args:
        creates: /etc/kubernetes/kubelet.conf
      register: join_result
    
    - name: Create .kube directory
      file:
        path: "/home/{{ k8s_username }}/.kube"
        state: directory
        owner: "{{ k8s_username }}"
        group: "{{ k8s_username }}"
        mode: '0755'
    
    - name: Copy kubeconfig from first control plane node
      shell: |
        scp -o StrictHostKeyChecking=no {{ hostvars[groups['k8s_master_init'][0]]['ansible_user'] }}@{{ hostvars[groups['k8s_master_init'][0]]['ansible_host'] }}:/home/{{ hostvars[groups['k8s_master_init'][0]]['ansible_user'] }}/.kube/config /home/{{ k8s_username }}/.kube/config
      args:
        creates: "/home/{{ k8s_username }}/.kube/config"
      become: true
      become_user: "{{ k8s_username }}"

# Configure HA components on all control plane nodes
- name: Configure HA Components
  hosts: k8s_master
  become: true
  tasks:
    - name: Install Keepalived and HAProxy
      apt:
        name:
          - keepalived
          - haproxy
        state: present
    
    - name: Configure Keepalived
      template:
        src: keepalived.conf.j2
        dest: /etc/keepalived/keepalived.conf
        mode: '0644'
      notify: Restart Keepalived
    
    - name: Create API server health check script
      template:
        src: check_apiserver.sh.j2
        dest: /etc/keepalived/check_apiserver.sh
        mode: '0755'
      notify: Restart Keepalived
    
    - name: Configure HAProxy
      template:
        src: haproxy.cfg.j2
        dest: /etc/haproxy/haproxy.cfg
        mode: '0644'
      notify: Restart HAProxy
    
    - name: Enable and start services
      systemd:
        name: "{{ item }}"
        state: started
        enabled: yes
      with_items:
        - keepalived
        - haproxy
        
    # Ensure kube-apiserver port configuration consistency across all nodes
    - name: Update kube-apiserver probe ports on all control plane nodes
      block:
        - name: Wait for kube-apiserver manifest to be created
          wait_for:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml
            state: present
            timeout: 60
          
        - name: Check if kube-apiserver manifest exists
          stat:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml
          register: api_manifest_stat
          
        - name: Check if kube-apiserver backup exists
          stat:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
          register: api_backup_stat
          when: not api_manifest_stat.stat.exists
          
        - name: Restore kube-apiserver manifest from backup if needed
          copy:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml
            remote_src: yes
            mode: '0644'
          when: not api_manifest_stat.stat.exists and api_backup_stat.stat.exists
          register: restored_api_manifest
          
        - name: Wait for API server to restart after restoring manifest
          pause:
            seconds: 30
          when: restored_api_manifest is changed
        
        - name: Backup kube-apiserver manifest
          copy:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
            remote_src: yes
            mode: '0644'
          register: api_backup
          
        - name: Copy kube-apiserver manifest for editing rather than moving
          copy:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml
            dest: /etc/kubernetes/kube-apiserver.yaml.tmp
            remote_src: yes
            mode: '0644'
          when: api_backup is succeeded
          
        - name: Update liveness probe port in kube-apiserver manifest
          replace:
            path: /etc/kubernetes/kube-apiserver.yaml.tmp
            regexp: '([ ]{8}port: )6443(\n[ ]{8}scheme: HTTPS)'
            replace: '\16444\2'
          when: api_backup is succeeded
          
        - name: Update readiness probe port in kube-apiserver manifest
          replace:
            path: /etc/kubernetes/kube-apiserver.yaml.tmp
            regexp: '([ ]{8}port: )6443(\n[ ]{8}scheme: HTTPS)'
            replace: '\16444\2'
          when: api_backup is succeeded
          
        - name: Update startup probe port in kube-apiserver manifest
          replace:
            path: /etc/kubernetes/kube-apiserver.yaml.tmp
            regexp: '([ ]{8}port: )6443(\n[ ]{8}scheme: HTTPS)'
            replace: '\16444\2'
          when: api_backup is succeeded
          
        - name: Update advertise address endpoint annotation if present
          replace:
            path: /etc/kubernetes/kube-apiserver.yaml.tmp
            regexp: '([ ]{4}kubeadm\.kubernetes\.io/kube-apiserver\.advertise-address\.endpoint: .*:)6443'
            replace: '\16444'
          when: api_backup is succeeded
          ignore_errors: true
          
        - name: Move kube-apiserver manifest back to trigger restart
          copy:
            src: /etc/kubernetes/kube-apiserver.yaml.tmp
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml
            remote_src: yes
            mode: '0644'
          when: api_backup is succeeded
          register: api_restart
          
        - name: Remove temporary manifest file
          file:
            path: /etc/kubernetes/kube-apiserver.yaml.tmp
            state: absent
          when: api_backup is succeeded
          
        - name: Ensure kube-apiserver manifest exists, restore from backup if needed
          block:
            - name: Check if kube-apiserver manifest exists after modifications
              stat:
                path: /etc/kubernetes/manifests/kube-apiserver.yaml
              register: final_api_manifest_stat
              
            - name: Restore from backup if manifest still missing
              copy:
                src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
                dest: /etc/kubernetes/manifests/kube-apiserver.yaml
                remote_src: yes
                mode: '0644'
              when: not final_api_manifest_stat.stat.exists
          rescue:
            - name: Emergency restore of kube-apiserver manifest from backup
              copy:
                src: /etc/kubernetes/manifests/kube-apiserver.yaml.bak
                dest: /etc/kubernetes/manifests/kube-apiserver.yaml
                remote_src: yes
                mode: '0644'
              ignore_errors: true
  
  handlers:
    - name: Restart Keepalived
      systemd:
        name: keepalived
        state: restarted
    
    - name: Restart HAProxy
      systemd:
        name: haproxy
        state: restarted
